{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training reviews:  25000\n",
      "Number of testing reviews:  25000\n",
      "Number of train labels:  25000\n",
      "Number of test labels:  25000\n"
     ]
    }
   ],
   "source": [
    "# Variables and data munging\n",
    "train_positive_reviews = []\n",
    "train_negative_reviews = []\n",
    "\n",
    "test_positive_reviews = []\n",
    "test_negative_reviews = []\n",
    "\n",
    "for filename_ in os.listdir('../data/aclImdb/train/pos'):\n",
    "    with open(os.path.join('../data/aclImdb/train/pos',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        train_positive_reviews.append(sentiment)\n",
    "    \n",
    "for filename_ in os.listdir('../data/aclImdb/train/neg'):\n",
    "    with open(os.path.join('../data/aclImdb/train/neg',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        train_negative_reviews.append(sentiment)\n",
    "    \n",
    "for filename_ in os.listdir('../data/aclImdb/test/pos'):\n",
    "    with open(os.path.join('../data/aclImdb/test/pos',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        test_positive_reviews.append(sentiment)\n",
    "    \n",
    "for filename_ in os.listdir('../data/aclImdb/test/neg'):\n",
    "    with open(os.path.join('../data/aclImdb/test/neg',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        test_negative_reviews.append(sentiment)\n",
    "\n",
    "train_reviews = train_positive_reviews + train_negative_reviews\n",
    "test_reviews = test_positive_reviews + test_negative_reviews\n",
    "\n",
    "train_labels = [0]*len(train_positive_reviews) + [1]*len(train_negative_reviews)\n",
    "test_labels = [0]*len(test_positive_reviews) + [1]*len(test_negative_reviews)\n",
    "\n",
    "print 'Number of training reviews: ', len(train_reviews)\n",
    "print 'Number of testing reviews: ', len(test_reviews)\n",
    "print 'Number of train labels: ', len(train_labels)\n",
    "print 'Number of test labels: ', len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  sentiment\n",
      "0  Kris Kristofferson, at his drugged-out peak in...          1\n",
      "1  I went in not knowing anything about this movi...          1\n",
      "2  I saw not so fabulous rating on IMDb, but I we...          1\n",
      "3  I got this DVD from a friend, who got it from ...          1\n",
      "4  An excellent performance by Alix Elias highlig...          1\n",
      "                                             reviews  sentiment\n",
      "0  Go to the video store and get the original. I ...          1\n",
      "1  SPOILER!! Terrible camera work, horrible writi...          1\n",
      "2  I went to see this movie mostly because it loo...          1\n",
      "3  A young woman, Nicole Carrow (Jaimie Alexander...          1\n",
      "4  This film is terribly bad. Kevin Spacey is a r...          1\n"
     ]
    }
   ],
   "source": [
    "# creating dataframes\n",
    "reviews_train_df = pd.DataFrame({'reviews': train_reviews, 'sentiment':train_labels})\n",
    "reviews_test_df = pd.DataFrame({'reviews': test_reviews, 'sentiment':test_labels})\n",
    "\n",
    "reviews_train_df = reviews_train_df.sample(frac=1).reset_index(drop=True)\n",
    "reviews_test_df = reviews_test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 1 for negative and 0 for positive\n",
    "print reviews_train_df.head()\n",
    "print reviews_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124259 unique tokens.\n",
      "(50000, 500)\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "reviews = train_reviews + test_reviews\n",
    "labels = train_labels + test_labels\n",
    "\n",
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# one hot encoding\n",
    "labels = np_utils.to_categorical(np.asarray(labels))\n",
    "\n",
    "print data.shape\n",
    "print labels.shape\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:25000]\n",
    "y_train = labels[:25000]\n",
    "x_val = data[25000:]\n",
    "y_val = labels[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../data/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM), dtype='float32')\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batchSize = 24\n",
    "numClasses = 2\n",
    "training_iters = 100000\n",
    "learning_rate = 0.001\n",
    "display_step = 20\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batchSize, MAX_SEQUENCE_LENGTH], name=\"input_x\")\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses], name=\"input_y\")\n",
    "data = tf.Variable(tf.zeros([batchSize, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM]), dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(embedding_matrix, input_data)\n",
    "\n",
    "conv1 = tf.layers.conv1d(data,filters=32,kernel_size=(3),padding=\"same\",activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=(2), strides=2)\n",
    "pool1_flat = tf.reshape(pool1, [-1, 8000])\n",
    "dense1 = tf.layers.dense(inputs=pool1_flat, units=250, activation=tf.nn.relu)\n",
    "out = tf.layers.dense(inputs=dense1, units=2, activation=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 0.449172, Training Accuracy= 0.91667\n",
      "Iter 480, Minibatch Loss= 0.938261, Training Accuracy= 0.37500\n",
      "Iter 960, Minibatch Loss= 1.016819, Training Accuracy= 0.29167\n",
      "Iter 1440, Minibatch Loss= 0.438288, Training Accuracy= 0.87500\n",
      "Iter 1920, Minibatch Loss= 0.814641, Training Accuracy= 0.50000\n",
      "Iter 2400, Minibatch Loss= 0.854882, Training Accuracy= 0.45833\n",
      "Iter 2880, Minibatch Loss= 0.688262, Training Accuracy= 0.62500\n",
      "Iter 3360, Minibatch Loss= 0.771595, Training Accuracy= 0.54167\n",
      "Iter 3840, Minibatch Loss= 0.896595, Training Accuracy= 0.41667\n",
      "Iter 4320, Minibatch Loss= 0.771595, Training Accuracy= 0.54167\n",
      "Iter 4800, Minibatch Loss= 0.896592, Training Accuracy= 0.41667\n",
      "Iter 5280, Minibatch Loss= 0.729928, Training Accuracy= 0.58333\n",
      "Iter 5760, Minibatch Loss= 0.771586, Training Accuracy= 0.54167\n",
      "Iter 6240, Minibatch Loss= 0.938254, Training Accuracy= 0.37500\n",
      "Iter 6720, Minibatch Loss= 0.771590, Training Accuracy= 0.54167\n",
      "Iter 7200, Minibatch Loss= 0.813262, Training Accuracy= 0.50000\n",
      "Iter 7680, Minibatch Loss= 0.771596, Training Accuracy= 0.54167\n",
      "Iter 8160, Minibatch Loss= 0.896595, Training Accuracy= 0.41667\n",
      "Iter 8640, Minibatch Loss= 0.854928, Training Accuracy= 0.45833\n",
      "Iter 9120, Minibatch Loss= 0.979928, Training Accuracy= 0.33333\n",
      "Iter 9600, Minibatch Loss= 0.854947, Training Accuracy= 0.45833\n",
      "Iter 10080, Minibatch Loss= 0.813262, Training Accuracy= 0.50000\n",
      "Iter 10560, Minibatch Loss= 0.813262, Training Accuracy= 0.50000\n",
      "Iter 11040, Minibatch Loss= 1.021594, Training Accuracy= 0.29167\n",
      "Iter 11520, Minibatch Loss= 0.813262, Training Accuracy= 0.50000\n",
      "Iter 12000, Minibatch Loss= 0.813232, Training Accuracy= 0.50000\n",
      "Iter 12480, Minibatch Loss= 0.896593, Training Accuracy= 0.41667\n",
      "Iter 12960, Minibatch Loss= 0.688262, Training Accuracy= 0.62500\n",
      "Iter 13440, Minibatch Loss= 0.813088, Training Accuracy= 0.50000\n",
      "Iter 13920, Minibatch Loss= 0.729928, Training Accuracy= 0.58333\n",
      "Iter 14400, Minibatch Loss= 0.854917, Training Accuracy= 0.45833\n",
      "Iter 14880, Minibatch Loss= 0.854928, Training Accuracy= 0.45833\n",
      "Iter 15360, Minibatch Loss= 0.688262, Training Accuracy= 0.62500\n",
      "Iter 15840, Minibatch Loss= 0.896595, Training Accuracy= 0.41667\n",
      "Iter 16320, Minibatch Loss= 0.979927, Training Accuracy= 0.33333\n",
      "Iter 16800, Minibatch Loss= 0.729928, Training Accuracy= 0.58333\n",
      "Iter 17280, Minibatch Loss= 0.771410, Training Accuracy= 0.54167\n",
      "Iter 17760, Minibatch Loss= 0.771595, Training Accuracy= 0.54167\n",
      "Iter 18240, Minibatch Loss= 0.938253, Training Accuracy= 0.37500\n",
      "Iter 18720, Minibatch Loss= 0.896594, Training Accuracy= 0.41667\n",
      "Iter 19200, Minibatch Loss= 0.813255, Training Accuracy= 0.50000\n",
      "Iter 19680, Minibatch Loss= 0.771595, Training Accuracy= 0.54167\n",
      "Iter 20160, Minibatch Loss= 0.979928, Training Accuracy= 0.33333\n",
      "Iter 20640, Minibatch Loss= 0.979925, Training Accuracy= 0.33333\n",
      "Iter 21120, Minibatch Loss= 0.938262, Training Accuracy= 0.37500\n",
      "Iter 21600, Minibatch Loss= 0.854915, Training Accuracy= 0.45833\n",
      "Iter 22080, Minibatch Loss= 0.688262, Training Accuracy= 0.62500\n",
      "Iter 22560, Minibatch Loss= 0.771510, Training Accuracy= 0.54167\n",
      "Iter 23040, Minibatch Loss= 0.895850, Training Accuracy= 0.41667\n",
      "Iter 23520, Minibatch Loss= 0.896597, Training Accuracy= 0.41667\n",
      "Iter 24000, Minibatch Loss= 0.853831, Training Accuracy= 0.37500\n",
      "Iter 24480, Minibatch Loss= 0.596693, Training Accuracy= 0.75000\n",
      "Iter 24960, Minibatch Loss= 0.725671, Training Accuracy= 0.58333\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (16, 500) for Tensor u'input_x:0', which has shape '(24, 500)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d4a818b4018a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             step*batchSize: ((step*batchSize) + batchSize)]\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Fit training using batch data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Calculate batch accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    973\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    976\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (16, 500) for Tensor u'input_x:0', which has shape '(24, 500)'"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(out,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batchSize < training_iters:\n",
    "        batch_xs, batch_ys = x_train[step*batchSize: ((step*batchSize) + batchSize)],y_train[\n",
    "            step*batchSize: ((step*batchSize) + batchSize)]\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={input_data: batch_xs, labels: batch_ys})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={input_data: batch_xs, labels: batch_ys})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={input_data: batch_xs, labels: batch_ys})\n",
    "            print \"Iter \" + str(step*batchSize) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
