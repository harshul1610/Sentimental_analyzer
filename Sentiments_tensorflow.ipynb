{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training reviews:  25000\n",
      "Number of testing reviews:  25000\n",
      "Number of train labels:  25000\n",
      "Number of test labels:  25000\n"
     ]
    }
   ],
   "source": [
    "# Variables and data munging\n",
    "train_positive_reviews = []\n",
    "train_negative_reviews = []\n",
    "\n",
    "test_positive_reviews = []\n",
    "test_negative_reviews = []\n",
    "\n",
    "for filename_ in os.listdir('../data/aclImdb/train/pos'):\n",
    "    with open(os.path.join('../data/aclImdb/train/pos',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        train_positive_reviews.append(sentiment)\n",
    "    \n",
    "for filename_ in os.listdir('../data/aclImdb/train/neg'):\n",
    "    with open(os.path.join('../data/aclImdb/train/neg',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        train_negative_reviews.append(sentiment)\n",
    "    \n",
    "for filename_ in os.listdir('../data/aclImdb/test/pos'):\n",
    "    with open(os.path.join('../data/aclImdb/test/pos',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        test_positive_reviews.append(sentiment)\n",
    "    \n",
    "for filename_ in os.listdir('../data/aclImdb/test/neg'):\n",
    "    with open(os.path.join('../data/aclImdb/test/neg',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        test_negative_reviews.append(sentiment)\n",
    "\n",
    "train_reviews = train_positive_reviews + train_negative_reviews\n",
    "test_reviews = test_positive_reviews + test_negative_reviews\n",
    "\n",
    "train_labels = [0]*len(train_positive_reviews) + [1]*len(train_negative_reviews)\n",
    "test_labels = [0]*len(test_positive_reviews) + [1]*len(test_negative_reviews)\n",
    "\n",
    "print 'Number of training reviews: ', len(train_reviews)\n",
    "print 'Number of testing reviews: ', len(test_reviews)\n",
    "print 'Number of train labels: ', len(train_labels)\n",
    "print 'Number of test labels: ', len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  sentiment\n",
      "0  What are people on here talking about? I must ...          1\n",
      "1  This is a fine drama and a nice change of pace...          0\n",
      "2  A thematic staple of cinema since its inceptio...          1\n",
      "3  I read somewhere where this film was supposed ...          1\n",
      "4  I am Black American and I loved this soap oper...          0\n",
      "                                             reviews  sentiment\n",
      "0  Beats me how people can describe this adolesce...          1\n",
      "1  Hitchcock is a great director. Ironically I mo...          1\n",
      "2  A trite fish-out-of-water story about two frie...          1\n",
      "3  I saw this superb documentary at the Santa Bar...          0\n",
      "4  Well, 1st off I haven't seen \"Silence of the L...          1\n"
     ]
    }
   ],
   "source": [
    "# creating dataframes\n",
    "reviews_train_df = pd.DataFrame({'reviews': train_reviews, 'sentiment':train_labels})\n",
    "reviews_test_df = pd.DataFrame({'reviews': test_reviews, 'sentiment':test_labels})\n",
    "\n",
    "reviews_train_df = reviews_train_df.sample(frac=1).reset_index(drop=True)\n",
    "reviews_test_df = reviews_test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 1 for negative and 0 for positive\n",
    "print reviews_train_df.head()\n",
    "print reviews_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124259 unique tokens.\n",
      "(50000, 500)\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "reviews = train_reviews + test_reviews\n",
    "labels = train_labels + test_labels\n",
    "\n",
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# one hot encoding\n",
    "labels = np_utils.to_categorical(np.asarray(labels))\n",
    "\n",
    "print data.shape\n",
    "print labels.shape\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:25000]\n",
    "y_train = labels[:25000]\n",
    "x_val = data[25000:]\n",
    "y_val = labels[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../data/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM), dtype='float32')\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batchSize = 128\n",
    "numClasses = 2\n",
    "training_iters = 25000\n",
    "learning_rate = 0.01\n",
    "display_step = 5\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batchSize, MAX_SEQUENCE_LENGTH], name=\"input_x\")\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses], name=\"input_y\")\n",
    "data = tf.Variable(tf.zeros([batchSize, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM]), dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(embedding_matrix, input_data)\n",
    "\n",
    "conv1 = tf.layers.conv1d(data,filters=32,kernel_size=(3),padding=\"same\",activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=(2), strides=2)\n",
    "pool1_flat = tf.reshape(pool1, [-1, 8000])\n",
    "dense1 = tf.layers.dense(inputs=pool1_flat, units=250, activation=tf.nn.relu)\n",
    "out = tf.layers.dense(inputs=dense1, units=2, activation=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 640, Minibatch Loss= 0.797637, Training Accuracy= 0.51562\n",
      "Iter 1280, Minibatch Loss= 0.844512, Training Accuracy= 0.46875\n",
      "Iter 1920, Minibatch Loss= 0.828887, Training Accuracy= 0.48438\n",
      "Iter 2560, Minibatch Loss= 0.742949, Training Accuracy= 0.57031\n",
      "Iter 3200, Minibatch Loss= 0.813262, Training Accuracy= 0.50000\n",
      "Iter 3840, Minibatch Loss= 0.852324, Training Accuracy= 0.46094\n",
      "Iter 4480, Minibatch Loss= 0.766387, Training Accuracy= 0.54688\n",
      "Iter 5120, Minibatch Loss= 0.813262, Training Accuracy= 0.50000\n",
      "Iter 5760, Minibatch Loss= 0.789824, Training Accuracy= 0.52344\n",
      "Iter 6400, Minibatch Loss= 0.875761, Training Accuracy= 0.43750\n",
      "Iter 7040, Minibatch Loss= 0.805449, Training Accuracy= 0.50781\n",
      "Iter 7680, Minibatch Loss= 0.828887, Training Accuracy= 0.48438\n",
      "Iter 8320, Minibatch Loss= 0.789824, Training Accuracy= 0.52344\n",
      "Iter 8960, Minibatch Loss= 0.828887, Training Accuracy= 0.48438\n",
      "Iter 9600, Minibatch Loss= 0.844511, Training Accuracy= 0.46875\n",
      "Iter 10240, Minibatch Loss= 0.891386, Training Accuracy= 0.42188\n",
      "Iter 10880, Minibatch Loss= 0.844512, Training Accuracy= 0.46875\n",
      "Iter 11520, Minibatch Loss= 0.828887, Training Accuracy= 0.48438\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(out,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while (step * batchSize) < training_iters-1:\n",
    "        batch_xs, batch_ys = x_train[(step-1)*batchSize: step*batchSize],y_train[(step-1)*batchSize: step*batchSize]\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={input_data: batch_xs, labels: batch_ys})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={input_data: batch_xs, labels: batch_ys})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={input_data: batch_xs, labels: batch_ys})\n",
    "            print \"Iter \" + str(step*batchSize) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
