{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables \n",
    "train_positive_reviews = []\n",
    "train_negative_reviews = []\n",
    "\n",
    "test_positive_reviews = []\n",
    "test_negative_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename_ in os.listdir('../data/aclImdb/train/pos'):\n",
    "    with open(os.path.join('../data/aclImdb/train/pos',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        train_positive_reviews.append(sentiment)\n",
    "    \n",
    "for filename_ in os.listdir('../data/aclImdb/train/neg'):\n",
    "    with open(os.path.join('../data/aclImdb/train/neg',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        train_negative_reviews.append(sentiment)\n",
    "    \n",
    "for filename_ in os.listdir('../data/aclImdb/test/pos'):\n",
    "    with open(os.path.join('../data/aclImdb/test/pos',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        test_positive_reviews.append(sentiment)\n",
    "    \n",
    "for filename_ in os.listdir('../data/aclImdb/test/neg'):\n",
    "    with open(os.path.join('../data/aclImdb/test/neg',filename_),'r') as f:\n",
    "        sentiment = f.read()\n",
    "        test_negative_reviews.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training reviews:  25000\n",
      "Number of testing reviews:  25000\n",
      "Number of train labels:  25000\n",
      "Number of test labels:  25000\n"
     ]
    }
   ],
   "source": [
    "train_reviews = train_positive_reviews + train_negative_reviews\n",
    "test_reviews = test_positive_reviews + test_negative_reviews\n",
    "\n",
    "train_labels = [0]*len(train_positive_reviews) + [1]*len(train_negative_reviews)\n",
    "test_labels = [0]*len(test_positive_reviews) + [1]*len(test_negative_reviews)\n",
    "\n",
    "print 'Number of training reviews: ', len(train_reviews)\n",
    "print 'Number of testing reviews: ', len(test_reviews)\n",
    "print 'Number of train labels: ', len(train_labels)\n",
    "print 'Number of test labels: ', len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  sentiment\n",
      "0  I may very well be one of the few who really s...          0\n",
      "1  so yes it is quite nostalgic watching the 1st ...          0\n",
      "2  And a self-admitted one to boot. At one point ...          1\n",
      "3  MAJOR LEAGUE: BACK TO THE MINORS (1998) Â½* <br...          1\n",
      "4  After some of the negative reviews i heard on ...          0\n",
      "                                             reviews  sentiment\n",
      "0  'Leatherheads' tries so hard. Tries to be ligh...          1\n",
      "1  I absolutely positively can't believe my fello...          1\n",
      "2  Just reading why this show got canceled makes ...          0\n",
      "3  A hilarious comedy by the best director ever, ...          0\n",
      "4  I feel conflicted about this film - it is one ...          1\n"
     ]
    }
   ],
   "source": [
    "reviews_train_df = pd.DataFrame({'reviews': train_reviews, 'sentiment':train_labels})\n",
    "reviews_test_df = pd.DataFrame({'reviews': test_reviews, 'sentiment':test_labels})\n",
    "\n",
    "reviews_train_df = reviews_train_df.sample(frac=1).reset_index(drop=True)\n",
    "reviews_test_df = reviews_test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 1 for negative and 0 for positive\n",
    "print reviews_train_df.head()\n",
    "print reviews_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def preprocess(sentence):\n",
    "#    list_of_word = text_to_word_sequence(sentence,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "#                                               lower=True,\n",
    "#                                               split=\" \")\n",
    "#    return list_of_word\n",
    "\n",
    "#reviews_train_df['words_reviews'] = reviews_train_df['reviews'].map(preprocess)\n",
    "#reviews_test_df['words_reviews'] = reviews_test_df['reviews'].map(preprocess)\n",
    "\n",
    "#reviews_train_df.head()\n",
    "#reviews_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124259 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "reviews = train_reviews + test_reviews\n",
    "labels = train_labels + test_labels\n",
    "\n",
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# one hot encoding\n",
    "labels = np_utils.to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 500)\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "print data.shape\n",
    "print labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:25000]\n",
    "y_train = labels[:25000]\n",
    "x_val = data[25000:]\n",
    "y_val = labels[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../data/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['verplank'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 100)          12426000  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 500, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 14,436,384\n",
      "Trainable params: 2,010,384\n",
      "Non-trainable params: 12,426,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "50s - loss: 0.6352 - acc: 0.6345 - val_loss: 0.4585 - val_acc: 0.7886\n",
      "Epoch 2/2\n",
      "49s - loss: 0.4091 - acc: 0.8173 - val_loss: 0.3976 - val_acc: 0.8195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13dc3a490>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
